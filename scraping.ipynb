{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def fetch_page(url):\n",
    "    \"\"\"Fetch HTML content from a given URL.\"\"\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    return None\n",
    "\n",
    "#Fetch Rendered HTML\n",
    "html_content = fetch_page(\"http://jobs.accel.com/jobs\")\n",
    "\n",
    "print(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "GEMINI_API_KEY = \"AIzaSyC6FRgWtdR4D_8ZrZq2pBnax1X0ZWxfPN0\"\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "def ai_extract_jobs(html):\n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert web scraper. Analyze the following HTML and **extract job-related data**.\n",
    "    \n",
    "    - Look for elements inside `<a>`, `<div>`, `<meta>`, and `<h4>` tags.\n",
    "    - Identify **job titles** from elements like `<div itemprop=\"title\">` or `<h4>`.\n",
    "    - Extract **job URLs** from `<a>` tags with attributes like `data-testid=\"job-title-link\"`.\n",
    "    - Parse **company names** from `<meta itemProp=\"description\">` or parent elements.\n",
    "    - Identify **job locations** if available.\n",
    "\n",
    "    **Return output as a structured JSON list:**\n",
    "    [\n",
    "      {{\"title\": \"Operations Associate\", \"company\": \"Gopuff\", \"url\": \"https://jobs.example.com/job1\", \"location\": \"Berlin, Germany\"}},\n",
    "      {{\"title\": \"Software Engineer\", \"company\": \"Stripe\", \"url\": \"https://jobs.example.com/job2\", \"location\": \"Remote\"}}\n",
    "    ]\n",
    "\n",
    "    Here is the HTML content:\n",
    "    {html}\n",
    "    \"\"\"\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "\n",
    "    raw_response = response.text\n",
    "\n",
    "    #print(\"RAW AI RESPONSE:\", response.text)\n",
    "    job_listings = process_raw_response(raw_response)\n",
    "    return job_listings\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "def process_raw_response(raw_response):\n",
    "    \"\"\"\n",
    "    Cleans and parses the raw response from the Gemini model.\n",
    "    \"\"\"\n",
    "    if not raw_response:\n",
    "        print(\"Error: raw_response is None or empty.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        cleaned_response = re.sub(r'^```json\\s*|\\s*```$', '', raw_response, flags=re.MULTILINE).strip()\n",
    "        if not cleaned_response.endswith(\"]\"):\n",
    "            cleaned_response = \"[\" + cleaned_response + \"]\"\n",
    "        cleaned_response = re.sub(r',\\s*}$', '}', cleaned_response) \n",
    "        #print(\"CLEANED RESPONSE:\", cleaned_response)\n",
    "        job_listings = json.loads(cleaned_response)\n",
    "        return job_listings\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"JSON Parsing Error:\", e)\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected Error:\", e)\n",
    "        return []\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_listing = ai_extract_jobs(html_content)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def setup_selenium():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\") \n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    driver = setup_selenium()\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the page to load completely\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "\n",
    "        # Get the page HTML\n",
    "        html = driver.page_source\n",
    "        driver.quit()\n",
    "\n",
    "        # Extract readable text using BeautifulSoup\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        text = soup.get_text(separator=\"\\n\", strip=True)  # Extract clean text\n",
    "\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        return f\"Error fetching {url}: {e}\"\n",
    "    \n",
    "job_listing = ai_extract_jobs(html_content)\n",
    "\n",
    "def process_job_urls(job_listings):\n",
    "    \"\"\"Extract URLs and scrape text from each job page.\"\"\"\n",
    "    job_urls = extract_job_urls(job_listings)\n",
    "    \n",
    "    extracted_texts = {}\n",
    "    for url in job_urls:\n",
    "        print(f\"Extracting text from: {url}\")\n",
    "        extracted_texts[url] = extract_text_from_url(url)\n",
    "\n",
    "    return extracted_texts\n",
    "\n",
    "def extract_job_urls(job_listings):\n",
    "    if not job_listings or not isinstance(job_listings, list):\n",
    "        print(\"No valid job listings found.\")\n",
    "        return []\n",
    "\n",
    "    #Extract URLs from job listings\n",
    "    job_urls = [job[\"url\"] for job in job_listings if \"url\" in job]\n",
    "    return job_urls\n",
    "\n",
    "\n",
    "#Extract and process text from job URLs\n",
    "job_texts = process_job_urls(job_listing)\n",
    "\n",
    "#Print extracted job texts\n",
    "print(\"\\n Extracted Text from Job Pages:\")\n",
    "for url, text in job_texts.items():\n",
    "    print(f\"\\n {url}\\n{text[:1000]}...\")  # Print first 1000 characters for preview\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
